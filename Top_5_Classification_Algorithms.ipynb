{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The explications in this notebook is from : https://medium.com/edureka/classification-algorithms-ba27044f28f1\n",
    "#### Naive Bayes, KNN, SVM codes are from : https://www.dataquest.io/blog/sci-kit-learn-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 Classification Algorithms Actually Used In Real Life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    " Logistic Regression is a classification and not a regression algorithm. It estimates discrete values (Binary values like 0/1, yes/no, true/false) based on a given set of independent variable(s).The values obtained would always lie within 0 and 1 since it predicts the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1563/1*jr5p0gOq9NiTyJ2sHY2kJw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1563/1*jr5p0gOq9NiTyJ2sHY2kJw.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model \n",
    "# import LogisticRegressionfrom sklearn.metrics \n",
    "# import accuracy_score \n",
    "\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X, y)\n",
    "# predicted_classes = model.predict(X)\n",
    "# accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "# parameters = model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different steps that could be tried in order to improve the model:\n",
    " * include interaction terms\n",
    " * remove features\n",
    " * regularize techniques\n",
    " * use a non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "With versatile features helping actualize both categorical and continuous dependent variables, it is a type of supervised learning algorithm mostly used for classification problems. What this algorithm does is, it splits the population into two or more homogeneous sets based on the most significant attributes making the groups as distinct as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1369/1*_1aCdXADQ9evTk_nHCas6g.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1369/1*_1aCdXADQ9evTk_nHCas6g.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, you can see that population is classified into four different groups based on multiple attributes to identify ‘if they will play or not’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# classifier = DecisionTreeClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! We can also use Decision Trees for regression problems. That is done with 'DecisionTreeRegressor()' !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "This is a classification technique based on an assumption of independence between predictors or what’s known as Bayes’ theorem. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "To build a Bayesian model is simple and particularly functional in case of enormous data sets. Along with simplicity, Naive Bayes is known to outperform sophisticated classification methods as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the necessary module\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# #create an object of the type GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "# #train the algorithm on training data and predict using the testing data\n",
    "# pred = gnb.fit(data_train, target_train).predict(data_test)\n",
    "# #print(pred.tolist())\n",
    "# #print the accuracy score of the model\n",
    "# print(\"Naive-Bayes accuracy : \",accuracy_score(target_test, pred, normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (k- Nearest Neighbors)\n",
    "\n",
    "K nearest neighbors is a simple algorithm used for both classification and regression problems. It basically stores all available cases to classify the new cases by a majority vote of its k neighbors. The case assigned to the class is most common amongst its K nearest neighbors measured by a distance function (Euclidean, Manhattan, Minkowski, and Hamming).\n",
    "\n",
    "While the three former distance functions are used for continuous variables, Hamming distance function is used for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1564/1*4FeRsslkfggjsYkdaGWZQw.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1564/1*4FeRsslkfggjsYkdaGWZQw.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import necessary modules\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# #create object of the lassifier\n",
    "# neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "# #Train the algorithm\n",
    "# neigh.fit(data_train, target_train)\n",
    "# # predict the response\n",
    "# pred = neigh.predict(data_test)\n",
    "# # evaluate accuracy\n",
    "# print (\"KNeighbors accuracy score : \",accuracy_score(target_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to consider before selecting KNN:\n",
    " * KNN is computationally expensive\n",
    " * Variables should be normalized else higher range variables can bias it\n",
    " * Works on pre-processing stage more before going for kNN like an outlier, noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(Support Vector Machine)\n",
    "\n",
    "In this algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate.\n",
    "\n",
    "For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two-dimensional space where each point has two coordinates (these coordinates are known as Support Vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/961/1*uEZKUTiBXbvx-YZezrR5tA.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://miro.medium.com/max/961/1*uEZKUTiBXbvx-YZezrR5tA.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/961/1*XlTtohrt8AidIafQX9uFag.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://miro.medium.com/max/961/1*XlTtohrt8AidIafQX9uFag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example shown above, the line which splits the data into two differently classified groups is the blue line, since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, that’s what class we can classify the new data as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import the necessary modules\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# #create an object of type LinearSVC\n",
    "# svc_model = LinearSVC(random_state=0)\n",
    "# #train the algorithm on training data and predict using the testing data\n",
    "# pred = svc_model.fit(data_train, target_train).predict(data_test)\n",
    "# #print the accuracy score of the model\n",
    "# print(\"LinearSVC accuracy : \",accuracy_score(target_test, pred, normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
